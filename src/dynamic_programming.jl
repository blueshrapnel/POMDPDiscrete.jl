#= Utility functions
=#

#= POMDPModelTools.policy_reward_vector(mdp, policy rewardfunction)
iterates through successor state distribution and calculates weighted sum
of rewards over all successor states, but does not take into account
an action distribution.
=#

# return rewards as a vector of the return type specified in reward()
function get_rewards(mdp::MDP, policy=nothing)
    # don't actually use the policy
    U = map(s -> reward(mdp, s), mdp.ùíÆ)  # returns a matrix of R(s)
end

# returns utility as a vector
function get_utility(mdp::MDP, policy::Policy)
    U = map(s -> value(policy, s), mdp.ùíÆ)  # returns a matrix of V(s)
end

"""
    initialise_vector(size; rng)

Returns a vector of the size given which is filled with zeros if no random number generator is specified, if the optinal argument `rng` is supplied, it uses the generator to return a vector of random values of the given size.
"""
function initialise_vector(size::Int; rng::Union{AbstractRNG,Nothing}=nothing)
    if isnothing(rng)
        return zeros(size)
    else   # argument checked rng <: AbstractRNG
        return rand(rng, size)
    end
end

#=
Policy Evaluation
=#

@doc raw"""
    policy_evaluation(mdp, œÄ)

Compute the value function by evaluating the policy in each state using the Bellman Expectation Equation. Sutton and Barto 2nd Edition p75
    ``` math
    V(s) = \sum_a\pi(a|s)\sum_{s^\prime, s} p(s^\prime, r |s, a) [r + \gamma v(s^\prime)]
    ```
"""
function policy_evaluation(mdp::Union{MDP,POMDP}, policy::StochasticPolicy)
    R = get_rewards(mdp)
    N‚Çõ = length(states(mdp))
    P = build_probabilistic_model(mdp) # only a function of the mdp
    T = reduce(hcat, [P[:, :, si] * policy.œÄ[si, :] for si in 1:N‚Çõ])' # policy is given and fixed

    # initialise value function to zero
    V = initialise_vector(N‚Çõ)
    while true  # may want to consider a max iters approach as well
        v = copy(V)
        V = R + mdp.Œ≥ * T * v
        Œî = abs.(v - V)
        maximum(Œî) < Œµ && break
        #@show maximum(Œî)
    end
    return V
end


"""
    value_iteration(mdp)

Find the optimal value function for an agent acting greedily.
"""
function value_iteration(mdp::Union{MDP,POMDP})
    N‚Çõ = length(states(mdp))
    # build a reward matrix with indices R[s',a,s]
    R = get_rewards(mdp)                    # as a column vector
    R_s_s‚Ä≤= repeat(R, 1, N‚Çõ)                # R[s, s']
    N‚Çê = length(actions(mdp))
    P = build_probabilistic_model(mdp)      # only a function of the mdp
    # initialise value function to zero
    V = initialise_vector(N‚Çõ)
    Œµ = 0.0001
    while true
        q‚Çõ = zeros(N‚Çê)
        Œî = 0
        for si ‚àà 1:N‚Çõ
            for ai ‚àà 1:N‚Çê
                # TODO q‚Çõ[ai] = sum(P[:,ai,si].*(R[:, ai, si] + mdp.Œ≥*V))
                # i.e. use R[s',a,s]
                q‚Çõ[ai] = sum(P[:,ai,si].*(R_s_s‚Ä≤[si, :] + mdp.Œ≥*V))
            end
            best_value = maximum(q‚Çõ)
            Œî = max(Œî, abs(V[si] - best_value))
            V[si] = best_value
        end
        Œî < Œµ && break
        #@show maximum(Œî)
    end
    return V
end
"""
    one_step_lookahead(si, P, R_s_s‚Ä≤, Œ≥, V, N‚Çê)

Returns q‚Çõ[ai] for the given state index `si`.  Requires as arguments the probabilistic model `P[s‚Ä≤, a, s]`, the full reward matrix `R[s, s‚Ä≤]`, the discount factor Œ≥ and the value function `V`.

Used as a helper function for `value iteration` and `greedy_policy`.
"""
function one_step_lookahead(si, P, R_s_s‚Ä≤, Œ≥, V, N‚Çê)
    q‚Çõ = zeros(N‚Çê)
    for ai ‚àà 1:N‚Çê
        # TODO q‚Çõ[ai] = sum(P[:,ai,si].*(R[:, ai, si] + mdp.Œ≥*V))
        # i.e. use R[s',a,s]
        q‚Çõ[ai] = sum(P[:,ai,si].*(R_s_s‚Ä≤[si, :] + Œ≥*V))
    end
    return q‚Çõ
end

# given the state action value for a state, return the best value and actions
function one_step_lookahead_actions(q‚Çõ)
    best_value = maximum(q‚Çõ)
    #best_actions = findall(q‚Çõ .‚âà best_value)
    best_actions = findall(q‚Çõ .‚âà best_value)
    return best_actions
end

# wrapper function to return the best_value
function one_step_lookahead_value(si, P, R_s_s‚Ä≤, Œ≥, V, N‚Çê)
    q‚Çõ = one_step_lookahead(si, P, R_s_s‚Ä≤, Œ≥, V, N‚Çê)
    best_value = maximum(q‚Çõ)
    return best_value
end

# wrapper function to return the best_value and best actions
function one_step_lookahead_value_actions(si, P, R_s_s‚Ä≤, Œ≥, V, N‚Çê)
    q‚Çõ = one_step_lookahead(si, P, R_s_s‚Ä≤, Œ≥, V, N‚Çê)
    best_value = maximum(q‚Çõ)
    best_actions = one_step_lookahead_actions(q‚Çõ)
    return best_value, best_actions
end

@doc raw"""
greedy_policy(mdp, value)

Return the deterministic optimal policy ``\pi \approx \pi^*`` such that ``\pi(s) = \arg \max_a \sum_{s', r} p(s',r|s,a)[r + \gamma V(s')]``.
"""
function greedy_policy(mdp::Union{MDP, POMDP}, V)
    N‚Çõ = length(states(mdp))
    N‚Çê = length(actions(mdp))
    R = get_rewards(mdp)                    # as a column vector
    R_s_s‚Ä≤= repeat(R, 1, N‚Çõ)                # R[s, s']
    N‚Çê = length(actions(mdp))
    P = build_probabilistic_model(mdp)      # only a function of the mdp

    œÄ = zeros(N‚Çõ, N‚Çê)
    for si ‚àà 1:N‚Çõ
        q‚Çõ =  one_step_lookahead(si, P, R_s_s‚Ä≤, mdp.Œ≥, V, N‚Çê)
        best_actions = one_step_lookahead_actions(q‚Çõ)
        for ai in best_actions
            œÄ[si, ai] = 1.0/length(best_actions)
        end
    end
    return œÄ
end


function update_state_action_value!(mdp, œÄ, Q)
    # Q: assume columns are action (output), rows are states (input)
    # œÄ: assume columns are action (output), rows are states (input)  
    N‚Çõ, N‚Çê = size(œÄ)
    for si ‚àà 1:N‚Çõ
        for ai ‚àà 1:N‚Çê
            ps‚Ä≤ = transition(mdp, si, ai)
            for (s‚Ä≤, p) in weighted_iterator(ps‚Ä≤)
                if p > 0.0
                    s‚Ä≤i = stateindex(mdp, s‚Ä≤)
                    q_si_s‚Ä≤i = 
                end    
                Q[si, ai] = sum(P[:,ai,si].*(R_s_s‚Ä≤[si, :] + mdp.Œ≥*V))
            end
        end
        best_value = maximum(q‚Çõ)
        Œî = max(Œî, abs(V[si] - best_value))
        V[si] = best_value
    end

end
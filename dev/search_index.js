var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = POMDPDiscrete","category":"page"},{"location":"#POMDPDiscrete","page":"Home","title":"POMDPDiscrete","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for POMDPDiscrete.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [POMDPDiscrete]","category":"page"},{"location":"#POMDPDiscrete.action_distribution-Tuple{POMDPDiscrete.StochasticPolicy, Any}","page":"Home","title":"POMDPDiscrete.action_distribution","text":"action_distribution(policy::StochasticPolicy, x)\n\nReturns the distribution of the ordered actions given the policy and the current state or belief x.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_grid-Tuple{GridWorld, POMDPs.Policy}","page":"Home","title":"POMDPDiscrete.policy_grid","text":"policy_grid(mdp, policy)\npolicy_grid(mdp, xmax, ymax)\n\nReturn a representation of the policy using unicode arrows stored in a Matrix{String}, where each state is represented by an element of the matrix.  This only shows one action per state, samples distribution to select action.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_transition_matrix-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}, Matrix{<:AbstractFloat}}","page":"Home","title":"POMDPDiscrete.policy_transition_matrix","text":"policy_transition_matrix(mdp, π)\n\nReturn a transition matrix given a probabilistic model Ps′, a, s) and a policy stochastic π[s, a].\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.random_stochastic_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.random_stochastic_policy","text":"random_stochastic_policy(mdp)\n\nReturn a stationary StochasticPolicy` where the action distribution for each state is random.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.uniform_stochastic_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.uniform_stochastic_policy","text":"uniform_stochastic_policy(mdp)\n\nReturn a stationary StochasticPolicy where all actions are equally likely for each state.\n\n\n\n\n\n","category":"method"}]
}

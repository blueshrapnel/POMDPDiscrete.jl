var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = POMDPDiscrete","category":"page"},{"location":"#POMDPDiscrete","page":"Home","title":"POMDPDiscrete","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for POMDPDiscrete.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [POMDPDiscrete]","category":"page"},{"location":"#POMDPDiscrete.action_distribution-Tuple{POMDPDiscrete.StochasticPolicy, Any}","page":"Home","title":"POMDPDiscrete.action_distribution","text":"action_distribution(policy::StochasticPolicy, x)\n\nReturns the distribution of the ordered actions given the policy and the current state or belief x.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.initialise_utility-Tuple{Int64}","page":"Home","title":"POMDPDiscrete.initialise_utility","text":"initialise_utility(size; rng)\n\nReturns a vector of the size given which is filled with zeros if no random number generator is specified, if the optinal argument rng is supplied, it uses the generator to return a vector of random values of the given size.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_evaluation-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}, POMDPDiscrete.StochasticPolicy}","page":"Home","title":"POMDPDiscrete.policy_evaluation","text":"policy_evaluation(mdp, π)\n\nCompute the value function by evaluating the policy in each state using the Bellman Expectation Equation. Sutton and Barto 2nd Edition p75     math     V(s) = \\sum_a\\pi(a|s)\\sum_{s^\\prime, s} p(s^\\prime, r |s, a) [r + \\gamma v(s^\\prime)]\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_grid-Tuple{GridWorld, POMDPs.Policy}","page":"Home","title":"POMDPDiscrete.policy_grid","text":"policy_grid(mdp, policy)\npolicy_grid(mdp, xmax, ymax)\n\nReturn a representation of the policy using unicode arrows stored in a Matrix{String}, where each state is represented by an element of the matrix.  This only shows one action per state, samples distribution to select action.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_transition_matrix-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}, Matrix{<:AbstractFloat}}","page":"Home","title":"POMDPDiscrete.policy_transition_matrix","text":"policy_transition_matrix(mdp, π)\n\nReturn a transition matrix given a probabilistic model Ps′, a, s) and a policy stochastic π[s, a].\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.random_stochastic_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.random_stochastic_policy","text":"random_stochastic_policy(mdp)\n\nReturn a stationary StochasticPolicy` where the action distribution for each state is random.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.uniform_stochastic_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.uniform_stochastic_policy","text":"uniform_stochastic_policy(mdp)\n\nReturn a stationary StochasticPolicy where all actions are equally likely for each state.\n\n\n\n\n\n","category":"method"}]
}

var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = POMDPDiscrete","category":"page"},{"location":"#POMDPDiscrete","page":"Home","title":"POMDPDiscrete","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for POMDPDiscrete.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [POMDPDiscrete]","category":"page"},{"location":"#POMDPDiscrete.action_distribution-Tuple{POMDPDiscrete.StochasticPolicy, Any}","page":"Home","title":"POMDPDiscrete.action_distribution","text":"action_distribution(policy::StochasticPolicy, x)\n\nReturns the distribution of the ordered actions given the policy and the current state or belief x.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.build_probabilistic_model-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.build_probabilistic_model","text":"build_probabilistic_model(mdp)\n\nReturn a 3 dimensional matrix representing P[s′, a, s], taking into account successor state distributions for action/state pairs.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.greedy_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}, Any}","page":"Home","title":"POMDPDiscrete.greedy_policy","text":"greedy_policy(mdp, value)\n\nReturn the deterministic optimal policy pi approx pi^* such that pi(s) = arg max_a sum_s r p(srsa)r + gamma V(s).\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.initialise_vector-Tuple{Int64}","page":"Home","title":"POMDPDiscrete.initialise_vector","text":"initialise_vector(size; rng)\n\nReturns a vector of the size given which is filled with zeros if no random number generator is specified, if the optinal argument rng is supplied, it uses the generator to return a vector of random values of the given size.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.one_step_lookahead-NTuple{6, Any}","page":"Home","title":"POMDPDiscrete.one_step_lookahead","text":"one_step_lookahead(si, P, R_s_s′, γ, V, Nₐ)\n\nReturns qₛ[ai] for the given state index si.  Requires as arguments the probabilistic model P[s′, a, s], the full reward matrix R[s, s′], the discount factor γ and the value function V.\n\nUsed as a helper function for value iteration and greedy_policy.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_evaluation-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}, POMDPDiscrete.StochasticPolicy}","page":"Home","title":"POMDPDiscrete.policy_evaluation","text":"policy_evaluation(mdp, π)\n\nCompute the value function by evaluating the policy in each state using the Bellman Expectation Equation. Sutton and Barto 2nd Edition p75     math     V(s) = \\sum_a\\pi(a|s)\\sum_{s^\\prime, s} p(s^\\prime, r |s, a) [r + \\gamma v(s^\\prime)]\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_grid-Tuple{GridWorld, POMDPs.Policy}","page":"Home","title":"POMDPDiscrete.policy_grid","text":"policy_grid(mdp, policy)\npolicy_grid(mdp, xmax, ymax)\n\nReturn a representation of the policy using unicode arrows stored in a Matrix{String}, where each state is represented by an element of the matrix.  This only shows one action per state, samples distribution to select action.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.policy_transition_matrix-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}, Matrix{<:Real}}","page":"Home","title":"POMDPDiscrete.policy_transition_matrix","text":"policy_transition_matrix(mdp, π)\n\nReturn a transition matrix given a probabilistic model Ps′, a, s) and a policy stochastic π[s, a].\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.random_stochastic_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.random_stochastic_policy","text":"random_stochastic_policy(mdp)\n\nReturn a stationary StochasticPolicy` where the action distribution for each state is random.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.uniform_stochastic_policy-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.uniform_stochastic_policy","text":"uniform_stochastic_policy(mdp)\n\nReturn a stationary StochasticPolicy where all actions are equally likely for each state.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPDiscrete.value_iteration-Tuple{Union{POMDPs.MDP, POMDPs.POMDP}}","page":"Home","title":"POMDPDiscrete.value_iteration","text":"value_iteration(mdp)\n\nFind the optimal value function for an agent acting greedily.\n\n\n\n\n\n","category":"method"},{"location":"#POMDPs.reward","page":"Home","title":"POMDPs.reward","text":"reward(mdp::GridWorld, s::State, [a::Any])\n\nReturns a reward of -1 for any action a taken in state s for a GridWorld mdp.  If the stsate s is in the set of absorbing states, then the reward is zero.  Note the reward is function of the current state only.\n\n\n\n\n\n","category":"function"},{"location":"#POMDPs.transition-Tuple{GridWorld, Int64, Int64}","page":"Home","title":"POMDPs.transition","text":"transition(mdp::GridWorld, s::State, a::Symbol)\n\nReturns the probability distribution of successor states for a GridWorld mdp, given the current state s and an action a selected in that state.\n\n\n\n\n\n","category":"method"}]
}

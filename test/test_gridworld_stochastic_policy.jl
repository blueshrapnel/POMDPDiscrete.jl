using POMDPDiscrete
import POMDPDiscrete.policy_transition_matrix

using POMDPs

import POMDPModelTools:ordered_actions

using Test

@testset "stochastic policy" begin
    mdp = GridWorld()
    random_policy = random_stochastic_policy(mdp)
    @test sum(random_policy.œÄ) ‚âà length(mdp.ùíÆ)

    uniform_policy = uniform_stochastic_policy(mdp)
    @test sum(uniform_policy.œÄ) ‚âà length(mdp.ùíÆ)
    for a in ordered_actions(mdp)
        i = actionindex(mdp, a)
        @test rand(uniform_policy.œÄ[:,i]) == 1/length(mdp.ùíú)
    end
end

@testset "policy_transition_matrix - 2x2 grid absorbing" begin
    mdp = GridWorld(
        size=(2,2),
        p_transition=1.0,
        absorbing_states=[State(1,1)],
        Œ≥=1.0)
    policy = uniform_stochastic_policy(mdp)
    T = policy_transition_matrix(mdp, policy)
    @test T == [1    0    0    0;
                0.25 0.5  0    0.25;
                0.25 0    0.5  0.25;
                0    0.25 0.25 0.5]
    # check successor state distribution sums to 1
    @test all(sum(T, dims=2) .‚âà 1)
end

@testset "policy_transition_matrix - stochastic grid" begin
    mdp = GridWorld(
        size=(3,7),
        p_transition=0.6,
        absorbing_states=State[],
        Œ≥=1.0)
    policy = random_stochastic_policy(mdp)
    T = policy_transition_matrix(mdp,policy)
    # expected value
    N‚Çõ = length(states(mdp))
    N‚Çê = length(actions(mdp))
    T‚ÇÇ = zeros(N‚Çõ, N‚Çõ)
    P = POMDPDiscrete.build_probabilistic_model(mdp)
    for si in 1:N‚Çõ
        for s‚Ä≤i in 1:N‚Çõ
            for ai in 1:N‚Çê
                T‚ÇÇ[si, s‚Ä≤i] += policy.œÄ[si, ai] * P[s‚Ä≤i, ai, si]
            end
        end
    end
    @test T ‚âà T‚ÇÇ
    # check successor state distribution sums to 1
    @test all(sum(T, dims=2) .‚âà 1)
end
